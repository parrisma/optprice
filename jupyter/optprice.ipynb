{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b97c38-59d0-4233-b2a0-4d76bf72a1c1",
   "metadata": {},
   "source": [
    "## Starting Point\n",
    "\n",
    "Neural Networks are excellent function approximators, so in a data rich world such as finance it would seem possible to train neural networks to learn these option pricing function. The question is **why** would we do this, when we have perfectly good and well established computer models that can already do it.\n",
    "\n",
    "## Terminology\n",
    "* **Option** a financial contract with a value contingent on some initial terms and how the financial markets move during the life of the contract.\n",
    "* **Model** a mathematical algorithm that can determine the value of a specific type of option.\n",
    "* **Neural Network** or **NN** A function approximator based on matrix manipulation that can be trained to find an approximate general form of a function given sufficient examples of input parameters and corresponding result(s).\n",
    "\n",
    "## Motivation\n",
    "\n",
    "There are many simple options that can be modelled *closed form*, i.e. with an equation, where we pass the valuation details and with relatively little compute cost and delay we are given a reliable valuation. \n",
    "\n",
    "However there are more complex options that have no such *closed form* as they have complex features. Typically these are *path dependent*, as in it matters how their valuation parameter change over time, and to solve these a very common solution is to run many simulations of how the valuation parameters will evolve in the future. We then valuation the option on all of these simulated future paths. It is then possible to combine the results of the simulations and establish the optimal price of the option. Where the *optimal* price is the price that would compensate the seller for entering into a contract where there is a likelihood they will have to pay out to tye buyer.\n",
    "\n",
    "In these path dependent case, the number of simulations needed to get an accurate price can be very high. As such the compute costs is high and the time to run the calculation is also long, relative to the *closed form* cases.\n",
    "\n",
    "Typically when selling (offering) options, the seller will make a market, by pricing many different terms and will also respond to **r**equest **f**or **q**uotes (RFQs) during the trading day. This can amount to a very significant number of calculations and as such significant compute costs eating into margins. In addition the time it can take to quite may be seen as client differentiating if it can be done more quickly than other sellers. In addition once a trade is done, the seller must re-value the trade as the market moves so they can understand their profit and loss as well as [hedge](https://www.investopedia.com/terms/h/hedge.asp) their risks. Again, this can involve a significant number of costly calculations.\n",
    "\n",
    "## Hypothesis  \n",
    "\n",
    "A neural network is slow to train but quick when making a prediction, as it is in essence a simple matrix multiplication. So the hypothesis we look at here is:\n",
    "> Can a neural network be trained to estimate the financial option contract pay-offs and be accurate enough to have commercial use.\n",
    "* If they can be trained is it more cost effective to train a neural network than to run it on demand as needed\n",
    "* and is the neural network quicker when predicting than the equivalent direct model call.\n",
    "* Then finally, are there viable commercial uses, proprietary trading, client valuations and approved in regulatory settings.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "* Normal Day to Day operations generate many option valuations, which if captured this data could be used as training data\n",
    "* Such, Day to Day data may not cover all possible scenarios, so might need to be augmented by creating specific training data\n",
    "* Some complex financial options have many features, and feature spaces of 50 to 100 would require a significant amount of training data to ensure coverage in high such dimensions.\n",
    "* Neural networks will always make a prediction, even when untrained.\n",
    "* From a client and regulatory point of view [expandability](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence) is a challenge as the seller may not be able to show why a certain price was given.\n",
    "* As the markets move, the neural will need re-training, however this can be done by re-training the existing neural network with additional data, but it needs to be monitored.\n",
    "* Accuracy will not be consistent, it will depend on where the prediction falls in feature space and how well covered that region is.\n",
    "* Availability of data, the sellers organization will need to be set-up to value data and its monetization. Otherwise the cost of souring the data and changing the organization mindset will be prohibitive.\n",
    "\n",
    "## Approach\n",
    "\n",
    "Train a Neural Net to predict a:\n",
    "\n",
    "1. Simple closed form model. This is not a target use case but is a first step in the proof.\n",
    "2. Simple closed form model, with a simulated barrier condition to see how the NN behaves with step changes in price\n",
    "3. Path dependent option with a small feature space.\n",
    "4. Extrapolate the results to consider\n",
    "   1. Data demands to train the neural network\n",
    "   2. Relative cost of compute, on-demand vs trained neural network\n",
    "   3. Relative speed of pricing \n",
    "   4. Pricing stability and accuracy\n",
    "   5. Operational challenges and re-training\n",
    "   6. Client concerns\n",
    "   7. Seller concerns\n",
    "   8. Regulatory concerns\n",
    "   9. Go, No-Go on continuing the experiment with a high feature NN or 50+ dimensions in feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff9d9a-f095-49ba-a2a2-5cfd29a7801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# General Python utilities and imports.\n",
    "#\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Final\n",
    "from typing import Tuple, List, Union, Callable\n",
    "from functools import partial\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdcd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Some set-up for the NN environment, for this we are using TensorFlow and Keras.\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib  # part of sklearn\n",
    "\n",
    "tf.keras.backend.set_floatx('float64') # We want high accuracy for our regressions, slower then float-32, but necessary for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49948bc7-e4ae-4e6d-9883-7390b5c7bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is so we can visualize results\n",
    "#\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad119fc-9626-4870-a812-538e7815d32a",
   "metadata": {},
   "source": [
    "## Simple Closed Form Model\n",
    "\n",
    "Our [closed form](https://en.wikipedia.org/wiki/Closed-form_expression) model is the [Black Scholes](https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model) option model. This will allow us to show the work flow and highlight the challenges with the approach.\n",
    "\n",
    "The specific model implementation below is arbitrary, it is just a means to show how the neural network can learn a function of the same order of magnitude of complexity as a simple option contract.\n",
    "\n",
    "The model also takes an function that inspects the price, volatility and Time to maturity and *optionally* modifies the price if it hit a condition in terms of price, volatility and time. This is to allow us to model the case where the option price has abrupt changes rather than smooth ones. Abrupt changes will be more of a challenge for the model to capture. We do this as many commercially traded options do have such [barrier](https://en.wikipedia.org/wiki/Barrier_option) features (knock-in, knock-out, up-and-out etc.)\n",
    "\n",
    "This naive implementation will run on the CPU only. There are techniques to utilize all CPU cores, however these vary by language C++, Python etc and are non trivial to get right and distract from the problem at hand. As we will see later using tensorflow, it is possible to create language independent computation graphs that can fully utilize the GPU(s) available. This is s significant step up as a contemporary GPU will have approx. 10'000 cores compared to a CPU with approx 16 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654fac00-de8a-4b9a-8189-23c099412250",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BSConstants:\n",
    "    call: Final[int] = 0\n",
    "    put: Final[int] = 1\n",
    "    price: Final[int] = 0\n",
    "    delta: Final[int] = 1\n",
    "\n",
    "\n",
    "def black_scholes_model(\n",
    "    S: float,\n",
    "    K: float,\n",
    "    T: float,\n",
    "    r: float,\n",
    "    v: float,\n",
    "    o: int = BSConstants.call,\n",
    "    barrier: Callable[[float, float, float, float], float] = None,\n",
    "    with_delta: bool = False,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Implementation of Black Scholes model.\n",
    "\n",
    "    Trivial example to demo of proof of concept for fitting an option pricing model\n",
    "    with a neural network.\n",
    "\n",
    "    :param S: Spot\n",
    "    :param K: Strike\n",
    "    :param T: Time to maturity\n",
    "    :param r: risk free rate\n",
    "    :param v: underlying volatility\n",
    "    :param o: type, call = 0 or put = 1\n",
    "    :param barrier: A callable that takes: price, spot, volatility and time and returns a modified price if a certain vol and/or time barrier has been hit\n",
    "    :param with_delta: If true calculate option delta, else return None for delta\n",
    "    :return: Option Price, Delta\n",
    "    \"\"\"\n",
    "    price: float = None\n",
    "    delta: float = None\n",
    "\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * v**2) * T) / (v * np.sqrt(T))\n",
    "    d2 = (np.log(S / K) + (r - 0.5 * v**2) * T) / (v * np.sqrt(T))\n",
    "\n",
    "    if o == BSConstants.call:\n",
    "        price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "        if with_delta:\n",
    "            delta = norm.cdf(d1)\n",
    "    elif o == BSConstants.put:\n",
    "        price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "        if with_delta:\n",
    "            delta = float(1) - norm.cdf(d1)\n",
    "    else:\n",
    "        assert False, \"Option must be call=0 or put=1\"\n",
    "\n",
    "    if barrier is not None:\n",
    "        return barrier(price, S, v, T)\n",
    "\n",
    "    return (price, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4796fa1a-4bff-4605-9cb2-79a2eab9771f",
   "metadata": {},
   "source": [
    "## Pricing Scenarios\n",
    "\n",
    "In the real use case we will collect the back history  [RFQ (request for quote)](https://www.investopedia.com/terms/r/request-for-quote.asp) as training data for the neural network. This presents the first challenge as to train the model we need a spread of data across all combinations of all parameters and instrument details that impact the price we are trying to predict. If we have gaps in our training data there will also be a weakness in the model when predicting prices with that specific combination of inputs.\n",
    "\n",
    "The nature of neural networks is to give results, even an totally untrained neural network will make predictions, just erroneous ones. So we will need to review the spread of inputs we have and make sure (expert judgment) that there are sufficient examples for all of the combinations we are likely to quote. If the markets shift significantly we can also add additional training data, but we must be aware when the model is being asked to make predictions outside of its experience.\n",
    "\n",
    "In this example we will create training data from scratch by running scenarios by varying the input parameters and capturing the results.\n",
    "\n",
    "**Note** \n",
    "\n",
    "* The Spot stock price is not that of any specific stock and the risk free rate is not that of a specific currency. Also, time is treated as a continuous variable, where zero = now. So the model is not linked to a specific stock, currency or time, so, in this simple case, we do not need to train our neural network for specific combinations of stock, currency & time as the NN will learn the general relationship as captured in the model.\n",
    "* However, stock prices and risk free rates vary by stock and currency, so we must make sure the training examples capture the full range for these values for all stock and rates we will encounter. \n",
    "* Also all inputs are floating point numbers, which do not respect when stocks trade at integer values and where stocks and rates are precision limited to e.g. cents and basis points. So we must interpret the result accordingly with market conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class MarketConstants:\n",
    "    volMinStep: Final[float] = 0.01\n",
    "    volMin: Final[float] = 0.01  # 1%\n",
    "    volMax: Final[float] = 0.4  # 40%\n",
    "    timeMinStep: Final[float] = (\n",
    "        1.0 / 365.0\n",
    "    )  # One day (technically not all days are trading days .. but)\n",
    "    timeMin: Final[float] = 1e-9\n",
    "    timeMax: Final[float] = 1.0  # One Year\n",
    "    spotMinStep: Final[float] = 1.0 / 100.0\n",
    "    spotMin: Final[float] = spotMinStep\n",
    "    spotMax: Final[float] = 100.0\n",
    "    strikeMinStep: Final[float] = spotMinStep\n",
    "    strikeMin: Final[float] = spotMin\n",
    "    strikeMax: Final[float] = spotMax\n",
    "    rateMinStep: Final[float] = 1.0 / 10000.0  # one basis point\n",
    "    rateMin: Final[float] = rateMinStep\n",
    "    rateMax: Final[float] = 0.25  # 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19211f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepped_range(\n",
    "        min_value: float,\n",
    "        max_value: float,\n",
    "        num_steps: float,\n",
    "        min_step_size: float = None\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "\n",
    "    Create a range of num_steps between min and max values. However if the required step size\n",
    "    is smaller than the given min_step_size, reduce the number of steps to align with the min\n",
    "    step size. The range returned is always bounded by the mix/max values given\n",
    "\n",
    "    :param min_value (float): the range start value\n",
    "    :param max_value (float): the range end value\n",
    "    :param num_steps (float): the number of steps to break the range into\n",
    "    :param min_step_size (float): the minimum step size\n",
    "    :return: a numpy array of the required steps\n",
    "\n",
    "    \"\"\"\n",
    "    assert max_value > min_value, \"Max must be greater then min\"\n",
    "    assert num_steps > 1, \"Number of steps must be greater than one\"\n",
    "\n",
    "    step_size = (max_value - min_value) / (num_steps - 1)\n",
    "    if min_step_size is not None:\n",
    "        if step_size < min_step_size:\n",
    "            step_size = min_step_size\n",
    "            num_steps = 1 + int((max_value - min_value) / step_size)\n",
    "\n",
    "    rng = []\n",
    "    for i in range(num_steps):\n",
    "        rng.append(min_value + (i * step_size))\n",
    "\n",
    "    return rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbf09b-b6fd-4916-95d8-78e63eafb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_scenarios(\n",
    "    vol_range: Tuple[float, float] = (\n",
    "        MarketConstants.volMin,\n",
    "        MarketConstants.volMax),\n",
    "    mat_range: Tuple[float, float] = (\n",
    "        MarketConstants.timeMin,\n",
    "        MarketConstants.timeMax),\n",
    "    spot_range: Tuple[float, float] = (\n",
    "        MarketConstants.spotMin,\n",
    "        MarketConstants.spotMax,),\n",
    "    strike_range: Tuple[float, float] = (\n",
    "        MarketConstants.strikeMin,\n",
    "        MarketConstants.strikeMax),\n",
    "    rate_range: Tuple[float, float] = (\n",
    "        MarketConstants.rateMin,\n",
    "        MarketConstants.rateMax),\n",
    "    num_steps=50,\n",
    "    simple=True\n",
    ") -> Tuple[List[float], List[float], List[float], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Create scenario vectors for volatility, maturity, spot, strike and risk-free rate\n",
    "\n",
    "    Note: In practice these parameters have natural upper and lower limits. This helps as we reduce the possible span\n",
    "          of feature space that we need to train in.\n",
    "\n",
    "    :param vol_range: The range to vary volatility over\n",
    "    :param mat_range: The range to vary maturity over\n",
    "    :param spot_range: The range to vary spot over\n",
    "    :param strike_range: The range to vary strike over\n",
    "    :param rate_range: The range to vary risk-free rate over\n",
    "    :param num_steps: number of scenario steps\n",
    "    :return: volatility, maturity, spot, strike and risk-free rate scenarios as list of float\n",
    "    \"\"\"\n",
    "    vols = stepped_range(\n",
    "        vol_range[0], vol_range[1], num_steps, None if simple else MarketConstants.volMinStep\n",
    "    )\n",
    "    mats = stepped_range(\n",
    "        mat_range[0], mat_range[1], num_steps, None if simple else MarketConstants.timeMinStep\n",
    "    )\n",
    "    spots = stepped_range(\n",
    "        spot_range[0], spot_range[1], num_steps, None if simple else MarketConstants.spotMinStep\n",
    "    )\n",
    "    strikes = stepped_range(\n",
    "        strike_range[0], strike_range[1], num_steps, None if simple else MarketConstants.strikeMinStep\n",
    "    )\n",
    "    rates = stepped_range(\n",
    "        rate_range[0], rate_range[1], num_steps, None if simple else MarketConstants.rateMinStep\n",
    "    )\n",
    "\n",
    "    return vols, mats, spots, strikes, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b027a3c-b41e-434f-b7f6-8d4f34579474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(\n",
    "    list1: Union[List[float], float],\n",
    "    list2: Union[List[float], float]\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Return all combinations of the given lists\n",
    "\n",
    "    :param list1: The first list\n",
    "    :param list2: The second list\n",
    "    :return: List of all combinations of lists 1 and List 2\n",
    "    \"\"\"\n",
    "    if not isinstance(list1, List):\n",
    "        list1 = [list1]\n",
    "    if not isinstance(list2, List):\n",
    "        list2 = [list2]\n",
    "    res = []\n",
    "    for x in list1:\n",
    "        for y in list2:\n",
    "            if not isinstance(x, List):\n",
    "                x = [x]\n",
    "            if not isinstance(y, List):\n",
    "                y = [y]\n",
    "            res.append([*x, *y])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03dcad-0705-4a2d-ac88-50a24de68063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scenarios(\n",
    "    spot: Union[List[float], float],\n",
    "    strike: Union[List[float], float],\n",
    "    mat: Union[List[float], float],\n",
    "    rate: Union[List[float], float],\n",
    "    vol: Union[List[float], float],\n",
    ") -> List[Tuple[float, float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Return a set of model scenario inputs based on given parameter scenarios\n",
    "\n",
    "    :param vol: List of volatilities or single volatility\n",
    "    :param vol: List of maturities or single maturity\n",
    "    :param vol: List of spots or single spot\n",
    "    :param vol: List of strike or single strike\n",
    "    :param vol: List of rates or single rate\n",
    "    :return: List of all combinations [[s,k,t,r,v]]\n",
    "    \"\"\"\n",
    "    return combinations(\n",
    "        combinations(combinations(combinations(spot, strike), mat), rate), vol\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0642a-065e-410a-aa8a-c8b7e5798b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_d_scenario(\n",
    "    spot: Union[List[float], float],\n",
    "    strike: Union[List[float], float],\n",
    "    mat: Union[List[float], float],\n",
    "    rate: Union[List[float], float],\n",
    "    vol: Union[List[float], float],\n",
    "    price_func: Callable,\n",
    "    scaler=None,\n",
    ") -> Tuple[List[float], str, List[float], str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate a scenario where two of the given parameters are scenarios\n",
    "\n",
    "    :param spot: spot scenario or single spot\n",
    "    :param strike: strike scenario or single strike\n",
    "    :param maturity: maturity scenario or single maturity\n",
    "    :param rate: rate scenario or single rate\n",
    "    :param vol: volatility scenario or single volatility\n",
    "    :param price_func: The pricing function, either black_scholes or model.predict\n",
    "    :param scaler: The scaler used to normalize the model inputs\n",
    "    :return: scenario List 1, scenario parameter 1 name, scenario List 2, scenario parameter 2 name, scenario prices\n",
    "\n",
    "    \"\"\"\n",
    "    params = [spot, strike, mat, rate, vol]\n",
    "    arg_names = [*inspect.signature(price_func).parameters.keys()]\n",
    "    scenario_params = [\n",
    "        [x[0][0], x[0][1], x[1]]\n",
    "        for x in zip(enumerate(params), arg_names)\n",
    "        if isinstance(x[0][1], List)\n",
    "    ]\n",
    "    assert (\n",
    "        len(scenario_params) == 2\n",
    "    ), \"Only two parameters can be passed as scenario lists\"\n",
    "    prices = np.zeros((len(scenario_params[0][1]), len(scenario_params[1][1])))\n",
    "    for i, x in enumerate(scenario_params[0][1]):\n",
    "        if price_func.__name__ != \"predict\":  # cell by cell\n",
    "            for j, y in enumerate(scenario_params[1][1]):\n",
    "                params[scenario_params[0][0]] = x\n",
    "                params[scenario_params[1][0]] = y\n",
    "                prices[i, j] = (price_func)(*params)[BSConstants.price]\n",
    "        else:  # row by row as it is quicker when calling model.predict\n",
    "            params[scenario_params[0][0]] = x\n",
    "            params[scenario_params[1][0]] = None\n",
    "            param_set = np.tile(np.asarray(params), (len(scenario_params[1][1]), 1))\n",
    "            param_set[:, scenario_params[1][0]] = scenario_params[1][1]\n",
    "            if scaler is not None:\n",
    "                param_set = scaler.transform(param_set)\n",
    "            prices[i, :] = (\n",
    "                (price_func)(param_set.astype(np.float64), verbose=0)\n",
    "            ).reshape(1, len(scenario_params[1][1]))\n",
    "        print(\n",
    "            \"{:0.0f} % Complete\".format(\n",
    "                100 * ((i * len(scenario_params[1][1])) / prices.size)\n",
    "            )\n",
    "        )\n",
    "    print(\"Done\")\n",
    "    return (\n",
    "        scenario_params[0][1],\n",
    "        scenario_params[0][2],\n",
    "        scenario_params[1][1],\n",
    "        scenario_params[1][2],\n",
    "        prices,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0dc78-19f1-43ee-97e6-125de7d975fd",
   "metadata": {},
   "source": [
    "### Generate parameter scenarios\n",
    "\n",
    "We don't have a database of previous quotes to use, so we will generate a range for each model parameter type. These will be used to generate training data by calling the model for each combination of the parameters. In this *toy* example we have the luxury of creating all the training data we need and ensuring that it is evenly distributed thought the feature space.\n",
    "\n",
    "In a real case we would try to collect this data as a side effect of where the real model was being used and only augment the training set where we observed gaps or thin spots in the training data. So with the real case, we would have to investigate the coverage of existing data in feature space and plug gaps as needed. We would also need to check for systematic shifts in the valuation parameters e.g. higher or lower prices, significant changes oin volatility, rates etc. In these case we would have to ensure the model was re-trained.\n",
    "\n",
    "Justing periodic re-training with data generated as part of RFQ process would not always be sufficient, especially if there were sudden shifts in the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108bc4d-3f43-4a6e-8245-e265ba978115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate a spread of parameter values - the will form the X vector of training data.\n",
    "#\n",
    "# Note that even for this toy example, we already have 3.2 million training examples, even for a relatively course grid over feature space.\n",
    "#\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=20)\n",
    "X = generate_model_scenarios(spot=spots, strike=strikes, mat=mats, rate=rates, vol=vols)\n",
    "print(f\"Generated {len(X):,} training scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483da04c-972f-48d8-abc0-f15a24406898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Value the option for each training X case, this will generate the Y value for training.\n",
    "#\n",
    "Y = [black_scholes_model(*params)[BSConstants.price] for params in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Shuffle training data, to prevent model getting stuck in local optima in a region of state space.\n",
    "#\n",
    "Xs, Ys = shuffle(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63bb42-5061-4755-9cd3-dcfd5a8262da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Merge results in a Panda frame so it is easier to manipulate\n",
    "#\n",
    "dfy = pd.DataFrame(Ys, columns=[\"price\"])\n",
    "dfx = pd.DataFrame(Xs, columns=[\"vols\", \"mats\", \"spots\", \"strikes\", \"rates\"])\n",
    "dfRaw = dfx.join(dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694a1a2-5dec-43c2-8be2-c0250b901475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save the raw data, so we can use later.\n",
    "#\n",
    "dfRaw.to_csv(\"XYRawSimple.csv\", encoding=\"ascii\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc92e2d-3b81-482d-b24c-6b5804856291",
   "metadata": {},
   "source": [
    "### A regression Neural network.\n",
    "\n",
    "This is where art meets science in that there are no hard and fast rules for the size, shape and architecture of a neural network that will be able to converge on a general form of the function in a given set of data (if one exists). There are guidelines and many types of layer that fit certain patterns for image processing etc, so there is always informed experimentation at this stage.\n",
    "\n",
    "This is a very simple model as the data set is relatively small, however even a simple data set such as this took a number of experiments to get the right balance.\n",
    "\n",
    "The model below has relatively few trainable parameters, but is deep (layers) by comparison as we need to capture the complexities function shape in all five dimensions of the feature (parameter) space. \n",
    "\n",
    "I did experiment with layer size, drop-outs etc, but in the end reducing the layer sizes and adding more layers gave the best result. With larger layer sizes the model\n",
    "had a tendency to over fit even when dropputs were added. For optimisation a standard Adam Optimiser and mean squared error as loss function, with a slight modified learning rate also gave a reasonable result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f088b-5416-4d42-b54e-85e5d6554d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_net():\n",
    "    \"\"\"\n",
    "    Create a Neural network with an architecture tuned to regression.\n",
    "\n",
    "    We chose a simple dense neural network here that is powerful enough to capture\n",
    "    a generalized form, but not too power full as to 'learn'the data and over fit.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a NN of 5 Dense layers, with 5 inputs (number of BS parameters) abd a single\n",
    "    # output that predicts the price for the given parameters\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Dense(32, input_shape=(5,), kernel_initializer=\"normal\", activation=\"tanh\")\n",
    "    )\n",
    "    model.add(Dense(16, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(8, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(4, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(2, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(1, kernel_initializer=\"normal\", activation=\"linear\"))\n",
    "\n",
    "    # Compile the model with the Adam optimizer, with a tuned step size.\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deeper_neural_net():\n",
    "    \"\"\"\n",
    "    Create a Neural network with an architecture tuned to regression and able to capture (generalize)\n",
    "    more complex features of closed-form model + barriers.\n",
    "\n",
    "    We chose a simple dense neural network here that is powerful enough to capture\n",
    "    a generalized form, but not too power full as to 'learn'the data and over fit.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a NN of 5 Dense layers, with 5 inputs (number of BS parameters) abd a single\n",
    "    # output that predicts the price for the given parameters\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Dense(32, input_shape=(5,), kernel_initializer=\"normal\", activation=\"tanh\")\n",
    "    )\n",
    "    model.add(Dense(16, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(8, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(8, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(4, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(4, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(2, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(2, kernel_initializer=\"normal\", activation=\"tanh\"))\n",
    "    model.add(Dense(1, kernel_initializer=\"normal\", activation=\"linear\"))\n",
    "\n",
    "    # Compile the model with the Adam optimizer, with a tuned step size.\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd3c9d-0c75-4d5e-846e-ea8002580379",
   "metadata": {},
   "source": [
    "### Fit the data\n",
    "\n",
    "Fitting can be switched between a standard dense network and a random forest. Both will learn to predict the price from the inputs with different internal approaches. We will investigate the differences when we look at how the predictions compare to the ideal as defined by the model we are fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00021cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    NEURAL_NET = 1\n",
    "    DEEP_NEURAL_NET = 2\n",
    "    RANDOM_FOREST_REGRESSOR = 3\n",
    "\n",
    "\n",
    "def fit(X: np.ndarray, Y: np.ndarray, model_type: ModelType, num_epoch: int = 50):\n",
    "    \"\"\"\n",
    "    Prepare the model inputs and fit the data with the given model type.\n",
    "\n",
    "    :param X: Pricing parameter scenarios\n",
    "    :param Y: Prices corresponding to given inputs\n",
    "    :param model_type: NEURAL_NET or RANDOM_FOREST_REGRESSOR\n",
    "    :return : The tf model, training history & test and training data used X_train, Y_train, X_test, Y_test\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into train & test\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    Y_train = np.asarray(Y_train)\n",
    "    Y_test = np.asarray(Y_test)\n",
    "\n",
    "    # Create a check point to save the model version with the best validation loss\n",
    "\n",
    "    saveBest = ModelCheckpoint(\n",
    "        filepath=\"best\", monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\"\n",
    "    )\n",
    "\n",
    "    # Fit to the requested type\n",
    "\n",
    "    if model_type == ModelType.NEURAL_NET:\n",
    "        print(\"Create and fit Neural Network\")\n",
    "        model = create_neural_net()\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            batch_size=256,\n",
    "            epochs=num_epoch,\n",
    "            verbose=1,\n",
    "            validation_data=(X_test, Y_test),\n",
    "            callbacks=[saveBest],\n",
    "        )\n",
    "        model.load_weights(\n",
    "            \"best\"\n",
    "        )  # load back the model corresponding to lowest validation loss\n",
    "    elif model_type == ModelType.DEEP_NEURAL_NET:\n",
    "        print(\"Create and fit Deep Neural Network\")\n",
    "        model = create_deeper_neural_net()\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            batch_size=256,\n",
    "            epochs=num_epoch,\n",
    "            verbose=1,\n",
    "            validation_data=(X_test, Y_test),\n",
    "            callbacks=[saveBest],\n",
    "        )\n",
    "        model.load_weights(\n",
    "            \"best\"\n",
    "        )  # load back the model corresponding to lowest validation loss\n",
    "    elif model_type == ModelType.RANDOM_FOREST_REGRESSOR:\n",
    "        print(\"Create and fit Random Forest Regressor\")\n",
    "        model = RandomForestRegressor(n_estimators=250, verbose=2)\n",
    "        model.fit(X_train, Y_train)\n",
    "        history = None\n",
    "    else:\n",
    "        assert (\n",
    "            False\n",
    "        ), \"Invalid model type specified, see model types defined by class ModelType\"\n",
    "\n",
    "    mse = mean_squared_error(model.predict(X_test), Y_test)\n",
    "    print(f\"Final Mean Square Error [{mse:5f}]\")\n",
    "    return model, history, X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59f860-9c11-41e2-9b72-d145e60b1ced",
   "metadata": {},
   "source": [
    "### Train the model by fitting to the given training data\n",
    "\n",
    "**Note** the neural network will mak predictions even **before** it is trained and at any stage during the training process. From the point of view of the neural network it does not return a confidence level in its prediction, our confidence in its predictions is based on us having trained the model on a representative set of data an achieved an task appropriate balance between fit and generalisation. This we can assess from the lowest loss and validation loss achieved as well as specific testing.\n",
    "\n",
    "An extension of this idea is that the neural network will make predictions for scenarios it has not been trained on, this happens when our feature space changes and we do not retain the model. Again the neural network will not generate an error or complain when it is asked to make a prediction outside of its experience. It falls to us to catch the need to retrain or prevent predictions from outside of its experience being requested by our application.\n",
    "\n",
    "So, before we train the model we will run some predictions so we can see this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fbde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-trained model, for demo.\n",
    "#\n",
    "model = create_neural_net()\n",
    "model.load_weights(\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partially train from shuffled data Xs, Ys - with limited epochs\n",
    "#\n",
    "model, history, X_train, Y_train, X_test, Y_test = fit(\n",
    "    Xs, Ys, ModelType.NEURAL_NET, num_epoch=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c522f7-ab68-4f65-9865-8f4a360df993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from shuffled data Xs, Ys\n",
    "#\n",
    "model, history, X_train, Y_train, X_test, Y_test = fit(\n",
    "    Xs, Ys, ModelType.DEEP_NEURAL_NET, num_epoch=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353d278",
   "metadata": {},
   "source": [
    "### Examine the training losses and verify if model has learned a generalisation of the option function.\n",
    "\n",
    "We expect to see the Loss (red line) drop off exponentially as the model learns (fits) the data and the error between actual and predicted reduces.\n",
    "\n",
    "The validation loss (blue line) should also drop off as the model get better at predicting for test data (hold out) that it has not seen during training. If the validation loss increases, this is an indication that the model is too powerful and has just learned the training data (over fitted) rather than fitting a generalized form of the function in the training data.\n",
    "\n",
    "The final validation loss is an indication of how accurate the model is. The accuracy is critical in determining if the model can be used in a commercial setting, however we must do more exhaustive testing to make sure the accuracy is preserved across the feature space.\n",
    "\n",
    "Note: the random forest regressor, does not have a training history as it works in a fundamentally different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52244151-81a1-4120-9e5c-245f506fb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(\n",
    "    loss: List[float], validation_loss: List[float], skip: int = 0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a two axis line graph of training and validation losses\n",
    "\n",
    "    :param loss            : Training losses per epoch\n",
    "    :param validation_ loss: Validation losses per epoch\n",
    "    :param skip            : don't plot the first skip points, this is because early loss values\n",
    "                             are typically and order of magnitude or two bigger than than the final\n",
    "                             ones. As such when plotted together it is not possible to see how the\n",
    "                             loss and validation loss behaved.\n",
    "    \"\"\"\n",
    "    _, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.set_xlabel(\"Training Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\", color=\"r\")\n",
    "    ax2.set_ylabel(\"Validation Loss\", color=\"b\")\n",
    "    ax1.plot(loss[skip:], color=\"r\")\n",
    "    ax2.plot(validation_loss[skip:], color=\"b\")\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93c873-91cb-4bae-a109-ad63d4cd4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random forest does not generate a training history\n",
    "#\n",
    "if history is not None:\n",
    "    plot_training_history(history.history[\"loss\"], history.history[\"val_loss\"], skip=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722fdaf-e8a9-4c6c-accd-b55d3c14ea97",
   "metadata": {},
   "source": [
    "### Verify the operation of the model\n",
    "\n",
    "We must verify the model is working across feature space. To do this we create a new set of scenarios and as the model to predict, we then compare to values generated by the original mathematical version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e22e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new set of test scenarios\n",
    "#\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2c315",
   "metadata": {},
   "source": [
    "**Note**\n",
    "There are three scenarios below, only run **one** of them at a time as they all return results to the same outputs. \n",
    "These outputs are used to plot the graphs that inspect the accuracy of the results. You can come back and run a different scenario and re-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02986cd2-f0ed-4f95-abf4-fa159a0b0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1 : Rate vs Vol Scenario\n",
    "#\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=100)\n",
    "s1, s1n, s2, s2n, actual = two_d_scenario(\n",
    "    spot=99, strike=100, mat=0.75, rate=rates, vol=vols, price_func=black_scholes_model\n",
    ")\n",
    "s1, s1n, s2, s2n, predicted = two_d_scenario(\n",
    "    spot=99, strike=100, mat=0.75, rate=rates, vol=vols, price_func=model.predict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2 : Maturity vs Vol Scenario\n",
    "#\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=100)\n",
    "s1, s1n, s2, s2n, actual = two_d_scenario(\n",
    "    spot=99, strike=100, mat=mats, rate=0.05, vol=vols, price_func=black_scholes_model\n",
    ")\n",
    "s1, s1n, s2, s2n, predicted = two_d_scenario(\n",
    "    spot=99, strike=100, mat=mats, rate=0.05, vol=vols, price_func=model.predict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3 : Spot vs Strike\n",
    "#\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=100)\n",
    "s1, s1n, s2, s2n, actual = two_d_scenario(\n",
    "    spot=spots,\n",
    "    strike=strikes,\n",
    "    mat=1.0,\n",
    "    rate=0.05,\n",
    "    vol=0.2,\n",
    "    price_func=black_scholes_model,\n",
    ")\n",
    "s1, s1n, s2, s2n, predicted = two_d_scenario(\n",
    "    spot=spots, strike=strikes, mat=1.0, rate=0.05, vol=0.2, price_func=model.predict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99af539-763c-4d5f-851b-1202b04cc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_surface(\n",
    "    xscen: np.ndarray,\n",
    "    yscen: np.ndarray,\n",
    "    actual: np.ndarray,\n",
    "    predicted: np.ndarray,\n",
    "    title: str,\n",
    "    xscen_lab: str,\n",
    "    yscen_lab: str,\n",
    "    azimuth: float = -45,\n",
    "    elevation: float = 30,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot dual surface of actual vs predicted, with a contour plot of price difference as %\n",
    "\n",
    "    :param xscen     : The scenario values for X Axis\n",
    "    :param yscen     : The scenario values for Y Axis\n",
    "    :param actual    : 2D Array of actual values for X and Y Scenarios\n",
    "    :param predicted : 2D Array of neural net predicted values for X and Y Scenarios\n",
    "    :param title     : Graph title\n",
    "    :param xscen_lab : X Axis label\n",
    "    :param yscen_lab : Y Axis label\n",
    "    \"\"\"\n",
    "    minz = min(np.min(actual), np.min(predicted))\n",
    "    maxz = max(np.max(actual), np.max(predicted))\n",
    "\n",
    "    X, Y = np.meshgrid(xscen, yscen)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.set_zlim3d(minz, maxz)\n",
    "    c1 = ax.contourf(\n",
    "        X,\n",
    "        Y,\n",
    "        ((actual - predicted) / np.maximum(1e-9, actual)) * 100.0,\n",
    "        levels=range(-100, 100, 1),\n",
    "        cmap=cm.RdGy,\n",
    "        offset=np.min(minz),\n",
    "    )\n",
    "    _ = ax.plot_surface(\n",
    "        X, Y, actual, cmap=cm.coolwarm, linewidth=0, edgecolor=\"none\", alpha=0.7\n",
    "    )\n",
    "    _ = ax.plot_surface(\n",
    "        X, Y, predicted, edgecolors=\"k\", linewidth=0.1, color=\"gray\", alpha=0.3\n",
    "    )\n",
    "    cax = fig.add_axes(\n",
    "        [\n",
    "            ax.get_position().x1 + 0.05,\n",
    "            ax.get_position().y0 + ((ax.get_position().height) * 0.15),\n",
    "            0.02,\n",
    "            (ax.get_position().height) * 0.7,\n",
    "        ]\n",
    "    )\n",
    "    cb = fig.colorbar(c1, cax=cax)\n",
    "    cb.ax.set_ylabel(\"% difference\", rotation=270)\n",
    "    ax.set_xlabel(xscen_lab)\n",
    "    ax.set_ylabel(yscen_lab)\n",
    "    ax.set_zlabel(\"Option Price\")\n",
    "    ax.set_title(title)\n",
    "    ax.view_init(azim=azimuth, elev=elevation)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af8864-e988-46ae-8505-3e4e1c0cfd79",
   "metadata": {},
   "source": [
    "## Actual vs Predicted\n",
    "\n",
    "### Surface\n",
    "\n",
    "We plot the actuals as a surface (blue-red) and then overlay the predicted as a translucent gray surface, so we can see where the two diverge. We also plot a contour of the % difference at every point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5d509-a3b5-4e5d-a3e1-36178bee4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results from one of the last scenarios above that was run.\n",
    "#\n",
    "plot_price_surface(\n",
    "    s1,\n",
    "    s2,\n",
    "    actual=actual,\n",
    "    predicted=predicted,\n",
    "    title=\"Predicted vs Actual Comparison\",\n",
    "    xscen_lab=s1n,\n",
    "    yscen_lab=s2n,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5063772-2abc-4b85-9391-3cfbe523680d",
   "metadata": {},
   "source": [
    "### Scatter \n",
    "If we scatter plot actual vs predicted then we would expect to see a tight line, where the actual and predicted are the same for given inputs. So the more diffused the line the lower the fit between actual and predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeecdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(\n",
    "    actual: List[float], predicted: List[float], title: str, x_lab: str, y_lab: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a scatter of actual vs predicted as a way to visualize differences.\n",
    "\n",
    "    :param actual    : 1D Array of actual priced\n",
    "    :param predicted : 1D Array of predicted\n",
    "    :param title     : Graph title\n",
    "    :param x_lab     : X Axis label\n",
    "    :param y_lab     : Y Axis label\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_lab)\n",
    "    plt.ylabel(y_lab)\n",
    "    plt.scatter(actual.flatten(), predicted.flatten(), s=0.25)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2396c-523b-45a4-9089-d4b7a80b9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted (if you plot actual/actual or predicted/predicted - you will see s straight line)\n",
    "#\n",
    "plot_scatter(\n",
    "    actual.flatten(),\n",
    "    predicted.flatten(),\n",
    "    x_lab=\"Actual\",\n",
    "    y_lab=\"Predicted\",\n",
    "    title=\"Accuracy Comparison\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144eca7",
   "metadata": {},
   "source": [
    "### Option Greeks, specifically Delta\n",
    "\n",
    "When trading options we need to know how the option behaves as the parameters change, Spot, Volatility etc .. these sensitivities to parameter changes are referred to as options greeks as they are named delta, gamma, vega, rho etc.\n",
    "\n",
    "A critical one to know is delta, which is the rate of change of the option price as the spot price changes. For the closed form model we are using here, there is a closed form function for delta. However in the more complex options delta is calculated numerically. This is done by calculating an option price for a given spot level, and a the option price foe very small increase in spot. (very small as in 1/1'000'000). From this we can calculate the rate of change of option price with respect to change in spot.\n",
    "\n",
    "We can use our trained network to generate the numerical delta and compare it to the closed form delta, this will be another indication of the viability of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4547f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_deltas = []\n",
    "closed_form_deltas = []\n",
    "\n",
    "small_change: Final[float] = 1e-6\n",
    "\n",
    "# Spot range must be within the trained range\n",
    "spots = range(1, 100)\n",
    "x_p1 = np.zeros((len(spots), 5))\n",
    "x_p2 = np.zeros((len(spots), 5))\n",
    "\n",
    "for i, spot in enumerate(spots, 0):\n",
    "    # Calc the closed form delta\n",
    "    _, delta = black_scholes_model(\n",
    "        S=spot, K=50, T=1.0, r=0.05, v=0.25, with_delta=True\n",
    "    )\n",
    "    closed_form_deltas.append(delta)\n",
    "    \n",
    "    # Build up the scenarios to make batch calls to model predict outside of this loop.\n",
    "    x_p1[i] = [spot, 50, 1.0, 0.05, 0.25]\n",
    "    x_p2[i] = [spot + small_change, 50, 1.0, 0.05, 0.25]\n",
    "\n",
    "# Predict as batches\n",
    "p1 = model.predict(x_p1, verbose=0)\n",
    "p2 = model.predict(x_p2, verbose=0)\n",
    "\n",
    "# Numerical delta is diff in price over spot increment\n",
    "numerical_deltas = ((p2-p1)/small_change).tolist()\n",
    "\n",
    "# Plot the results vs the closed form to make it easy to visualize\n",
    "plt.clf()\n",
    "plt.title(f\"Numerical NN Delta vs Closed form Delta\")\n",
    "plt.plot(spots, closed_form_deltas, color=\"blue\", label=\"Closed Form\")\n",
    "plt.plot(spots, numerical_deltas, color=\"red\", label=\"NN Numeric\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff934c17",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see that it is possible to train a neural network for a financial contracts with a well behaved pay-off. We now add in an arbitrary feature that makes the pay-off harder to predict and type again.\n",
    "\n",
    "The acid test, is that if we quoted options from the Neural Network, would we be over/under pricing, in which case we would be exposing an arbitrage opportunity to the market. In this case the market would be able to make mostly risk free profit from trading with us by monetising the miss-pricing. This would systematically costs us money to trade and ultimately we would be driven out of business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478fcb89",
   "metadata": {},
   "source": [
    "### Best Case ...\n",
    "\n",
    "We need to take a moment to consider the <sup>1</sup> joke below\n",
    "\n",
    "> Theres this farmer, and he has these chickens, but they wont lay any eggs. So, he calls a physicist to help. The physicist then does some calculations, and he says, um, I have a solution, but it only works with spherical chickens in a vacuum\n",
    "\n",
    "Our *spherical chicken in a vacuum* is the Nobel winning [Black Scholes Model](https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model), while ground breaking, out of the box it represents a best case, where our option is a spherical chicken in a vacuum, with a well behaved pay off. In practice there are many embellishments required to accommodate dividends, early exercise, barriers, cross currency (quanto) etc. These embellishments make the pay off less well behaved in mathematical terms. This is relevant to us, as approximating functions with complex, sharp or instantaneous transitions with a large number of parameters is harder then approximating functions with a small number of parameters and smooth transitions. The real world use case where an options have a barrier conditions, where within a given range of pricing inputs the option behavior changes, sometimes instantaneously to zero value.\n",
    "\n",
    "We can gain an intuition for this complexity by looking at Fourier transforms, which is another way of approximating functions. There is a way of modelling a sharp transition (step function) using Fourier transforms. However to effectively model this instantaneous transition we must allow ourselves to use functions of infinitely high frequency. Clearly this is not practical, so limit ourselves to finite frequencies that yield a workable level of accuracy. There is a similar trade off with neural networks, we cannot have infinitely complex networks, we must try to find the least complexity that can capture and generalise the payoff with a tradable level of accuracy and precision.\n",
    "\n",
    "In the cells below, we show how fourier transforms work and compare five frequency terms versus one hundred frequency terms. Where more terms give us a very close approximation, but still an approximation.\n",
    "\n",
    "The analogy with neural networks is that as the real world complexity rises, we require a significant amount of data and deep networks to capture the payoff.\n",
    "\n",
    "So, let's have a look at the Fourier transform as a means to gain some intuition into this challenge.\n",
    "\n",
    "<sup>1</sup> *Credit [Big Bang Theory](https://isntsciencewonderful.wordpress.com/2015/09/06/hofstadters-spherical-chickens/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_approx_of_step_function(num_fourier_terms: int = 5):\n",
    "    \"\"\"\n",
    "    Calculate the first n terms of the fourier transform that approximates a cyclic step function.\n",
    "\n",
    "    |---|   |---|\n",
    "    |   |   |   |\n",
    "    |   |---|   |---\n",
    "\n",
    "    https://en.wikipedia.org/wiki/Fourier_series\n",
    "\n",
    "    Keyword Arguments:\n",
    "        num_fourier_terms -- number of fourier terms to include (default: {5})\n",
    "    \"\"\"\n",
    "    num_increments = 100\n",
    "    vals = np.asarray([(x / 100.0) * 4 * np.pi for x in range(num_increments)])\n",
    "    res = np.asarray(\n",
    "        [\n",
    "            [(4 * np.sin(n * (v))) / (np.pi * n) for v in vals]\n",
    "            for n in range(1, (num_fourier_terms * 2), 2)\n",
    "        ]\n",
    "    )\n",
    "    return vals, res, res.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_step_fnx(num_fourier_terms: int = 5):\n",
    "    \"\"\"\n",
    "    Calculate and plot all of the sub terms of the Fourier transform for a step function.\n",
    "    Plot the sub terms in grey & the final approximation (sum) in Red\n",
    "\n",
    "    Keyword Arguments:\n",
    "        num_fourier_terms -- number of fourier terms to include (default: {5})\n",
    "    \"\"\"\n",
    "    vals, res, apx = fourier_approx_of_step_function(\n",
    "        num_fourier_terms=num_fourier_terms\n",
    "    )\n",
    "    plt.clf()\n",
    "    plt.title(f\"{num_fourier_terms} terms\")\n",
    "    for i, r in enumerate(res):\n",
    "        plt.plot(vals, r, color=\"gray\", linewidth=max(0.0125, np.power(0.7, i)))\n",
    "    plt.plot(vals, apx, color=\"red\", linewidth=1)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see the higher the number of terms the more closely we can approximate the step function.\n",
    "# The implications are that the neural network has to work *harder* to approximate functions with\n",
    "# sharp transitions. Where there sharp transitions are very common in the day to day of financial options\n",
    "#\n",
    "# In grey we can see all of the sub-terms, in red we can see the shape of the function we get if we add\n",
    "# the sub terms together\n",
    "#\n",
    "plot_step_fnx(5)\n",
    "plot_step_fnx(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f958c4",
   "metadata": {},
   "source": [
    "So, now we have an intuition for the challenge in capturing complexity, we will add real world features to our option and see what we need to do the neural network model and supply of training data to try and capture this more complex form.\n",
    "\n",
    "We start by adding barrier conditions, where the payoff changes for a defined range of inputs. We compare two types of barrier, one with smooth transitions and one with instantaneous ones, the latter, as above we expect to be harder to capture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e65ae",
   "metadata": {},
   "source": [
    "### Smooth Barrier\n",
    "\n",
    "We now model a barrier that has a rapid but smooth transition for the option price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These settings are arbitrary, and just chosen to place the barrier in the center\n",
    "# of the range of spot prices, so it is easy to visualise when graphed.\n",
    "#\n",
    "# In practice, the barriers are normally selected by the buyer to fit a strategy they\n",
    "# have in mind to protect against loss of speculate against expected market moves.\n",
    "#\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=20)\n",
    "\n",
    "price_max = spots[-1]\n",
    "price_min = spots[0]\n",
    "barrier_min = (\n",
    "    price_max - price_min\n",
    ") * 0.45  # Barrier kicks in at 45% of the max option price\n",
    "barrier_max = (\n",
    "    price_max - price_min\n",
    ") * 0.8  # Barrier end at 80% of the max option price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_barrier(\n",
    "    p: float,\n",
    "    S: float,\n",
    "    v: float,\n",
    "    T: float,\n",
    "    barrier_spot_min: float,\n",
    "    barrier_spot_max: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Takes the current option price and reduces it smoothly, but sharply if the spot price in the the min/bax barrier range.\n",
    "\n",
    "    :param p: Unmodified option price\n",
    "    :param s: spot used to generate the given unmodified option price\n",
    "    :param v: vol used to generate the given unmodified option price\n",
    "    :param T: Maturity used to generate the given unmodified option price\n",
    "    :param barrier_spot_min: The spot price above which the barrier operator is active\n",
    "    :param barrier_spot_max: The spot price below which the barrier operator is active\n",
    "    :return: Modified option price.\n",
    "    \"\"\"\n",
    "    scale_down = float(1)\n",
    "    if S >= barrier_spot_min and S <= barrier_spot_max:\n",
    "        scale_down = 1 - np.sin(\n",
    "            ((S - barrier_spot_min) / (barrier_spot_max - barrier_spot_min)) * np.pi\n",
    "        )\n",
    "\n",
    "    return p * scale_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752aaa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-fill the additional params to barrier, so function conforms with our defined barrier prototype\n",
    "#\n",
    "smb = partial(\n",
    "    smooth_barrier, barrier_spot_min=barrier_min, barrier_spot_max=barrier_max\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eec976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick simulation to show shape of barrier\n",
    "#\n",
    "opt_prc = []\n",
    "spot_prc = []\n",
    "for s in range(1, 100):\n",
    "    opt_prc.append(\n",
    "        black_scholes_model(S=s, K=10, T=1.0, r=0.05, v=0.3, o=0, barrier=smb)\n",
    "    )\n",
    "    spot_prc.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2779c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(\n",
    "    x: List[float], y: List[float], title: str = \"\", x_lab: str = \"\", y_lab: str = \"\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Simple line plot of a single variable.\n",
    "\n",
    "    :param x         : 1D Array of x values to plot.\n",
    "    :param y         : 1D Array of y values to plot.\n",
    "    :param title     : Graph title\n",
    "    :param x_lab     : X Axis label\n",
    "    :param y_lab     : Y Axis label\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_lab)\n",
    "    plt.ylabel(y_lab)\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the pay-off, with the addition of smooth barrier.\n",
    "# We can see the payoff drop when the price in within the barrier range.\n",
    "#\n",
    "plot_line(\n",
    "    x=spot_prc,\n",
    "    y=opt_prc,\n",
    "    title=\"Smooth Barrier - pay-off\",\n",
    "    x_lab=\"Spot Price\",\n",
    "    y_lab=\"Option Price\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76afae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate a fresh set of training data for barrier type 1 = smooth barrier.\n",
    "#\n",
    "\n",
    "X_b1 = generate_model_scenarios(\n",
    "    spot=spots, strike=strikes, mat=mats, rate=rates, vol=vols\n",
    ")\n",
    "print(f\"Generated {len(X_b1):,} training scenarios\")\n",
    "Y_b1 = [black_scholes_model(*params, barrier=smb) for params in X_b1]\n",
    "Xs_b1, Ys_b1 = shuffle(X_b1, Y_b1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now fit the new training data with the same model architecture and see if it can (cope) generalise the more complex pay off.\n",
    "#\n",
    "model_b1, history_b1, X_train_b1, Y_train_b1, X_test_b1, Y_test_b1 = fit(\n",
    "    Xs_b1, Ys_b1, ModelType.NEURAL_NET\n",
    ")\n",
    "if history_b1 is not None:\n",
    "    plot_training_history(\n",
    "        history_b1.history[\"loss\"], history_b1.history[\"val_loss\"], skip=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate : Spot vs Vol for Smooth barrier\n",
    "#\n",
    "def bsmb(S: float, K: float, T: float, r: float, v: float, o: float = float(0)):\n",
    "    return black_scholes_model(S, K, T, r, v, o, barrier=smb)\n",
    "\n",
    "\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=100)\n",
    "s1, s1n, s2, s2n, actual = two_d_scenario(\n",
    "    spot=spots, strike=20.0, mat=1.0, rate=0.05, vol=vols, price_func=bsmb\n",
    ")\n",
    "s1, s1n, s2, s2n, predicted = two_d_scenario(\n",
    "    spot=spots,\n",
    "    strike=20.0,\n",
    "    mat=1.0,\n",
    "    rate=0.05,\n",
    "    vol=vols,\n",
    "    price_func=model_b1.predict,\n",
    "    scaler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_price_surface(\n",
    "    vols,  # Can swap these next two to see different scenarios\n",
    "    mats,\n",
    "    actual=actual,\n",
    "    predicted=predicted,\n",
    "    title=\"Predicted vs Actual Comparison\",\n",
    "    xscen_lab=s1n,\n",
    "    yscen_lab=s2n,\n",
    ")\n",
    "plot_scatter(\n",
    "    actual.flatten(),\n",
    "    predicted.flatten(),\n",
    "    x_lab=\"Actual\",\n",
    "    y_lab=\"Predicted\",\n",
    "    title=\"Accuracy Comparison\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce69405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dual_line(\n",
    "    x: List[float],\n",
    "    y1: List[float],\n",
    "    y2: List[float],\n",
    "    title: str = \"\",\n",
    "    x_lab: str = \"\",\n",
    "    y1_lab: str = \"\",\n",
    "    y2_lab: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Simple dual line plot.\n",
    "\n",
    "    :param x         : Common 1D Array of x values to plot for y1 and y2\n",
    "    :param y1        : First 1D Array of y values to plot.\n",
    "    :param y2        : Second 1D Array of y values to plot.\n",
    "    :param title     : Graph title\n",
    "    :param x_lab     : X Axis label\n",
    "    :param y1_lab    : y1 Axis label\n",
    "    :param y2_lab    : y2 Axis label\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_xlabel(x_lab)\n",
    "    ax1.plot(x, y1, color=\"green\")\n",
    "    ax2.plot(x, y2, color=\"red\")\n",
    "    ax1.set_ylabel(y1_lab)\n",
    "    ax2.set_ylabel(y2_lab)\n",
    "    ax1.legend([y1_lab], loc=0)\n",
    "    ax2.legend([y2_lab], loc=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a930d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simulation to show shape of predicted price with barrier\n",
    "#\n",
    "opt_prc_pred = []\n",
    "spot_prc = []\n",
    "for s in range(1, 100):\n",
    "    xs = np.asarray([s, 10, 1.0, 0.05, 0.3]).reshape((1, 5))\n",
    "    opt_prc_pred.append(model_b1.predict(xs, verbose=0)[0])\n",
    "    spot_prc.append(s)\n",
    "\n",
    "plot_dual_line(\n",
    "    x=spot_prc,\n",
    "    y1=opt_prc,\n",
    "    y2=opt_prc_pred,\n",
    "    title=\"Smooth Barrier Actual vs Predicted\",\n",
    "    x_lab=\"Spot Price\",\n",
    "    y1_lab=\"Actual Option Price\",\n",
    "    y2_lab=\"Predicted Option Price\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689732d",
   "metadata": {},
   "source": [
    "### Step Barrier\n",
    "\n",
    "We now try training with a barrier that has a sharp transition to zero, instantaneous transitions are difficult to model but are common in financial options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce958766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_barrier(\n",
    "    p: float,\n",
    "    S: float,\n",
    "    v: float,\n",
    "    T: float,\n",
    "    barrier_spot_min: float,\n",
    "    barrier_spot_max: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Takes the current option price and reduces it to zero between spot min and max\n",
    "\n",
    "    :param p: Unmodified option price\n",
    "    :param s: spot used to generate the given unmodified option price\n",
    "    :param v: vol used to generate the given unmodified option price\n",
    "    :param T: Maturity used to generate the given unmodified option price\n",
    "    :param barrier_spot_min: The spot price above which the barrier operator is active\n",
    "    :param barrier_spot_max: The spot price below which the barrier operator is active\n",
    "    :return: Modified option price.\n",
    "    \"\"\"\n",
    "    scale_down = float(1)\n",
    "    if S >= barrier_spot_min and S <= barrier_spot_max:\n",
    "        scale_down = float(0)\n",
    "\n",
    "    return p * scale_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-fill the additional params to barrier, so function conforms with expected prototype\n",
    "#\n",
    "sms = partial(step_barrier, barrier_spot_min=barrier_min, barrier_spot_max=barrier_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296106a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick simulation to show shape of barrier\n",
    "#\n",
    "opt_prc = []\n",
    "spot_prc = []\n",
    "for s in range(1, 100):\n",
    "    opt_prc.append(\n",
    "        black_scholes_model(S=s, K=10, T=1.0, r=0.05, v=0.3, o=0, barrier=sms)\n",
    "    )\n",
    "    spot_prc.append(s)\n",
    "\n",
    "plot_line(\n",
    "    x=spot_prc,\n",
    "    y=opt_prc,\n",
    "    title=\"Step Barrier\",\n",
    "    x_lab=\"Spot Price\",\n",
    "    y_lab=\"Option Price\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fee8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate a fresh set of training data for barrier type 2 = step barrier.\n",
    "#\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=25)\n",
    "X_b2 = generate_model_scenarios(\n",
    "    spot=spots, strike=strikes, mat=mats, rate=rates, vol=vols\n",
    ")\n",
    "print(f\"Generated {len(X_b2)} training scenarios\")\n",
    "Y_b2 = [black_scholes_model(*params, barrier=sms) for params in X_b2]\n",
    "Xs_b2, Ys_b2 = shuffle(X_b2, Y_b2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now fit the new training data with the same model architecture and see if it can (cope) generalise the more complex pay off.\n",
    "#\n",
    "\n",
    "# model_b2, history_b2, X_train_b2, Y_train_b2, X_test_b2, Y_test_b2 = fit(\n",
    "#    Xs_b2, Ys_b2, ModelType.NEURAL_NET)\n",
    "\n",
    "model_b2, history_b2, X_train_b2, Y_train_b2, X_test_b2, Y_test_b2 = fit(\n",
    "    Xs_b2, Ys_b2, ModelType.DEEP_NEURAL_NET\n",
    ")\n",
    "if history_b2 is not None:\n",
    "    plot_training_history(\n",
    "        history_b2.history[\"loss\"], history_b2.history[\"val_loss\"], skip=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate : Spot vs Vol for Step Barrier\n",
    "#\n",
    "def bsms(S: float, K: float, T: float, r: float, v: float, o: float = float(0)):\n",
    "    return black_scholes_model(S, K, T, r, v, o, barrier=sms)\n",
    "\n",
    "\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=100)\n",
    "\n",
    "s1, s1n, s2, s2n, actual = two_d_scenario(\n",
    "    spot=spots, strike=20.0, mat=1.0, rate=0.05, vol=vols, price_func=bsms\n",
    ")\n",
    "s1, s1n, s2, s2n, predicted = two_d_scenario(\n",
    "    spot=spots,\n",
    "    strike=20.0,\n",
    "    mat=1.0,\n",
    "    rate=0.05,\n",
    "    vol=vols,\n",
    "    price_func=model_b2.predict,\n",
    "    scaler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc08fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_price_surface(\n",
    "    vols,  # Can swap these next two to see different scenarios\n",
    "    mats,\n",
    "    actual=actual,\n",
    "    predicted=predicted,\n",
    "    title=\"Predicted vs Actual Comparison\",\n",
    "    xscen_lab=s1n,\n",
    "    yscen_lab=s2n,\n",
    ")\n",
    "\n",
    "plot_scatter(\n",
    "    actual.flatten(),\n",
    "    predicted.flatten(),\n",
    "    x_lab=\"Actual\",\n",
    "    y_lab=\"Predicted\",\n",
    "    title=\"Accuracy Comparison\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simulation to show shape of predicted price with barrier\n",
    "#\n",
    "opt_prc_pred = []\n",
    "spot_prc = []\n",
    "for s in range(1, 100):\n",
    "    xs = np.asarray([s, 10, 1.0, 0.05, 0.3]).reshape((1, 5))\n",
    "    opt_prc_pred.append(model_b2.predict(xs, verbose=0)[0])\n",
    "    spot_prc.append(s)\n",
    "\n",
    "plot_dual_line(\n",
    "    x=spot_prc,\n",
    "    y1=opt_prc,\n",
    "    y2=opt_prc_pred,\n",
    "    title=\"Step Barrier Actual vs Predicted\",\n",
    "    x_lab=\"Spot Price\",\n",
    "    y1_lab=\"Actual Option Price\",\n",
    "    y2_lab=\"Predicted Option Price\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb04f4",
   "metadata": {},
   "source": [
    "## Path Dependant\n",
    "\n",
    "Now we have completed a basic proof of concept with simple closed form, we move on to look at path dependant options with a much higher cost of compute and an even more complex payoff function.\n",
    "\n",
    "To get going we have taken a path dependant option from an open source git repo, that is implemented as a tensor flow model that can run on a GPU. We need this as calculating millions of path dependant option values for training takes days or elapse time, even with a GPU.\n",
    "\n",
    "This type of option allows for an early exercise, where the buyer can exit the contract before the defined maturity date. As such this is another type of feature for the neural network to learn.\n",
    "\n",
    "So, we follow the same pattern, by creating a set of all possible features, which comes to more than 12 million points even on a fairly corse grid. The value of the option for each scenario is then calculated. This gives us a training data set, but this time it is expensive to calculate and as the payoff is more complex we need more data. This highlights the trend that will be a challenge for this approach, the more complex the option, the more data we need which is now more expensive to compute. Which is why we try to use past quotes (calculations) as we have these for free. However they may not evenly cover the feature space we need to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeaae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# https://github.com/google/tf-quant-finance/blob/master/tf_quant_finance/black_scholes/crr_binomial_tree_test.py\n",
    "#\n",
    "\n",
    "\n",
    "def _get_payoff_fn(strikes, is_call_options):\n",
    "    \"\"\"Constructs the payoff functions.\"\"\"\n",
    "    option_signs = tf.cast(is_call_options, dtype=strikes.dtype) * 2 - 1\n",
    "\n",
    "    def payoff(spots):\n",
    "        \"\"\"Computes payff for the specified options given the spot grid.\n",
    "\n",
    "        Args:\n",
    "          spots: Tensor of shape [batch_size, grid_size, 1]. The spot values at some\n",
    "            time.\n",
    "\n",
    "        Returns:\n",
    "          Payoffs for exercise at the specified strikes.\n",
    "        \"\"\"\n",
    "        return tf.nn.relu((spots - strikes) * option_signs)\n",
    "\n",
    "    return payoff\n",
    "\n",
    "\n",
    "def _get_value_modifier(is_american, payoff_fn):\n",
    "    \"\"\"Constructs the value modifier for american style exercise.\"\"\"\n",
    "\n",
    "    def modifier(values, spots):\n",
    "        immediate_exercise_value = payoff_fn(spots)\n",
    "        return tf.where(\n",
    "            is_american, tf.math.maximum(immediate_exercise_value, values), values\n",
    "        )\n",
    "\n",
    "    return modifier\n",
    "\n",
    "\n",
    "def option_price_binomial(\n",
    "    *,\n",
    "    volatilities,\n",
    "    strikes,\n",
    "    expiries,\n",
    "    spots,\n",
    "    discount_rates=None,\n",
    "    dividend_rates=None,\n",
    "    is_call_options=None,\n",
    "    is_american=None,\n",
    "    num_steps=100,\n",
    "    dtype=None,\n",
    "    name=None,\n",
    "):\n",
    "    \"\"\"Computes the BS price for a batch of European or American options.\n",
    "\n",
    "    Uses the Cox-Ross-Rubinstein version of the binomial tree method to compute\n",
    "    the price of American or European options. Supports batching of the options\n",
    "    and allows mixing of European and American style exercises in a batch.\n",
    "    For more information about the binomial tree method and the\n",
    "    Cox-Ross-Rubinstein method in particular see the references below.\n",
    "\n",
    "    #### Example\n",
    "\n",
    "    ```python\n",
    "    # Prices 5 options with a mix of Call/Put, American/European features\n",
    "    # in a single batch.\n",
    "    dtype = np.float64\n",
    "    spots = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=dtype)\n",
    "    strikes = np.array([3.0, 3.0, 3.0, 3.0, 3.0], dtype=dtype)\n",
    "    volatilities = np.array([0.1, 0.22, 0.32, 0.01, 0.4], dtype=dtype)\n",
    "    is_call_options = np.array([True, True, False, False, False])\n",
    "    is_american = np.array([False, True, True, False, True])\n",
    "    discount_rates = np.array(0.035, dtype=dtype)\n",
    "    dividend_rates = np.array([0.02, 0.0, 0.07, 0.01, 0.0], dtype=dtype)\n",
    "    expiries = np.array(1.0, dtype=dtype)\n",
    "\n",
    "    prices = option_price_binomial(\n",
    "        volatilities=volatilities,\n",
    "        strikes=strikes,\n",
    "        expiries=expiries,\n",
    "        spots=spots,\n",
    "        discount_rates=discount_rates,\n",
    "        dividend_rates=dividend_rates,\n",
    "        is_call_options=is_call_options,\n",
    "        is_american=is_american,\n",
    "        dtype=dtype)\n",
    "    # Prints [0., 0.0098847, 0.41299509, 0., 0.06046989]\n",
    "    ```\n",
    "\n",
    "    #### References\n",
    "\n",
    "    [1] Hull, John C., Options, Futures and Other Derivatives. Pearson, 2018.\n",
    "    [2] Wikipedia contributors. Binomial Options Pricing Model. Available at:\n",
    "      https://en.wikipedia.org/wiki/Binomial_options_pricing_model\n",
    "\n",
    "    Args:\n",
    "      volatilities: Real `Tensor` of any shape and dtype. The volatilities to\n",
    "        expiry of the options to price.\n",
    "      strikes: A real `Tensor` of the same dtype and compatible shape as\n",
    "        `volatilities`. The strikes of the options to be priced.\n",
    "      expiries: A real `Tensor` of same dtype and compatible shape as\n",
    "        `volatilities`. The expiry of each option. The units should be such that\n",
    "        `expiry * volatility**2` is dimensionless.\n",
    "      spots: A real `Tensor` of any shape that broadcasts to the shape of the\n",
    "        `volatilities`. The current spot price of the underlying.\n",
    "      discount_rates: An optional real `Tensor` of same dtype as the\n",
    "        `volatilities`. The risk free discount rate. If None the rate is assumed\n",
    "        to be 0.\n",
    "        Default value: None, equivalent to discount rates = 0..\n",
    "      dividend_rates: An optional real `Tensor` of same dtype as the\n",
    "        `volatilities`. If None the rate is assumed to be 0.\n",
    "        Default value: None, equivalent to discount rates = 1.\n",
    "      is_call_options: A boolean `Tensor` of a shape compatible with\n",
    "        `volatilities`. Indicates whether the option is a call (if True) or a put\n",
    "        (if False). If not supplied, call options are assumed.\n",
    "        Default value: None, equivalent to is_call_options = True.\n",
    "      is_american: A boolean `Tensor` of a shape compatible with `volatilities`.\n",
    "        Indicates whether the option exercise style is American (if True) or\n",
    "        European (if False). If not supplied, European style exercise is assumed.\n",
    "        Default value: None, equivalent to is_american = False.\n",
    "      num_steps: A positive scalar int32 `Tensor`. The size of the time\n",
    "        discretization to use.\n",
    "        Default value: 100.\n",
    "      dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n",
    "        of any supplied non-`Tensor` arguments to `Tensor`.\n",
    "        Default value: None which maps to the default dtype inferred by TensorFlow\n",
    "          (float32).\n",
    "      name: str. The name for the ops created by this function.\n",
    "        Default value: None which is mapped to the default name `option_price`.\n",
    "\n",
    "    Returns:\n",
    "      A `Tensor` of the same shape as the inferred batch shape of the input data.\n",
    "      The Black Scholes price of the options computed on a binomial tree.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name or \"crr_option_price\"):\n",
    "        strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\")\n",
    "        dtype = strikes.dtype\n",
    "        volatilities = tf.convert_to_tensor(\n",
    "            volatilities, dtype=dtype, name=\"volatilities\"\n",
    "        )\n",
    "        expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\"expiries\")\n",
    "        spots = tf.convert_to_tensor(spots, dtype=dtype, name=\"spots\")\n",
    "\n",
    "        if discount_rates is None:\n",
    "            discount_rates = tf.zeros_like(volatilities)\n",
    "        else:\n",
    "            discount_rates = tf.convert_to_tensor(\n",
    "                discount_rates, dtype=dtype, name=\"discount_rates\"\n",
    "            )\n",
    "        if dividend_rates is None:\n",
    "            dividend_rates = tf.zeros_like(volatilities)\n",
    "        else:\n",
    "            dividend_rates = tf.convert_to_tensor(\n",
    "                dividend_rates, dtype=dtype, name=\"dividend_rates\"\n",
    "            )\n",
    "        if is_call_options is None:\n",
    "            is_call_options = tf.ones_like(\n",
    "                volatilities, dtype=tf.bool, name=\"is_call_options\"\n",
    "            )\n",
    "        else:\n",
    "            is_call_options = tf.convert_to_tensor(\n",
    "                is_call_options, dtype=tf.bool, name=\"is_call_options\"\n",
    "            )\n",
    "        if is_american is None:\n",
    "            is_american = tf.zeros_like(volatilities, dtype=tf.bool, name=\"is_american\")\n",
    "        else:\n",
    "            is_american = tf.convert_to_tensor(\n",
    "                is_american, dtype=tf.bool, name=\"is_american\"\n",
    "            )\n",
    "\n",
    "        num_steps = tf.cast(num_steps, dtype=dtype)\n",
    "        dt = expiries / num_steps\n",
    "\n",
    "        # CRR choices for the up and down move multipliers\n",
    "        ln_up = volatilities * tf.math.sqrt(dt)\n",
    "        ln_dn = -ln_up\n",
    "\n",
    "        # Prepares the spot grid.\n",
    "        grid_idx = tf.range(num_steps + 1)\n",
    "        # Stores the grid as shape [input_batch, N + 1] where N = num_steps.\n",
    "        log_spot_grid_1 = tf.expand_dims(\n",
    "            tf.math.log(spots) + ln_up * num_steps, axis=-1\n",
    "        )\n",
    "        log_spot_grid_2 = tf.expand_dims(ln_dn - ln_up, axis=-1) * grid_idx\n",
    "        log_spot_grid = log_spot_grid_1 + log_spot_grid_2\n",
    "\n",
    "        # Adding the new dimension is to ensure that batch shape is at the front.\n",
    "        payoff_fn = _get_payoff_fn(\n",
    "            tf.expand_dims(strikes, axis=-1), tf.expand_dims(is_call_options, axis=-1)\n",
    "        )\n",
    "        value_mod_fn = _get_value_modifier(\n",
    "            tf.expand_dims(is_american, axis=-1), payoff_fn\n",
    "        )\n",
    "\n",
    "        # Shape [batch shape, num time steps + 1]\n",
    "        values_grid = payoff_fn(tf.math.exp(log_spot_grid))\n",
    "\n",
    "        p_up = tf.math.exp((discount_rates - dividend_rates) * dt + ln_up) - 1\n",
    "        p_up /= tf.math.exp(2 * ln_up) - 1\n",
    "        p_up = tf.expand_dims(p_up, axis=-1)\n",
    "        p_dn = 1 - p_up\n",
    "        discount_factors = tf.expand_dims(tf.math.exp(-discount_rates * dt), axis=-1)\n",
    "        ln_up = tf.expand_dims(ln_up, axis=-1)\n",
    "\n",
    "        def one_step_back(current_values, current_log_spot_grid):\n",
    "            next_values = (\n",
    "                current_values[..., 1:] * p_dn + current_values[..., :-1] * p_up\n",
    "            )\n",
    "            next_log_spot_grid = current_log_spot_grid[..., :-1] - ln_up\n",
    "            next_values = value_mod_fn(next_values, tf.math.exp(next_log_spot_grid))\n",
    "            return discount_factors * next_values, next_log_spot_grid\n",
    "\n",
    "        def should_continue(current_values, current_log_spot_grid):\n",
    "            del current_values, current_log_spot_grid\n",
    "            return True\n",
    "\n",
    "        batch_shape = values_grid.shape[:-1]\n",
    "        pv, _ = tf.while_loop(\n",
    "            should_continue,\n",
    "            one_step_back,\n",
    "            (values_grid, log_spot_grid),\n",
    "            maximum_iterations=tf.cast(num_steps, dtype=tf.int32),\n",
    "            shape_invariants=(\n",
    "                tf.TensorShape(batch_shape + [None]),\n",
    "                tf.TensorShape(batch_shape + [None]),\n",
    "            ),\n",
    "        )\n",
    "        return tf.where(\n",
    "            expiries > 0,\n",
    "            tf.squeeze(pv, axis=-1),\n",
    "            tf.where(\n",
    "                is_call_options,\n",
    "                tf.math.maximum(spots - strikes, 0),\n",
    "                tf.math.maximum(strikes - spots, 0),\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4907207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Simple test to show the model can be tuned to price the same as the simple model\n",
    "#\n",
    "\n",
    "path_res = []\n",
    "cfrm_res = []\n",
    "vols = []\n",
    "\n",
    "v = 0.05\n",
    "S = 3.0\n",
    "K = 3.0\n",
    "T = 1.0\n",
    "r = 0.035\n",
    "\n",
    "for i in range(100):\n",
    "    vols.append(v)\n",
    "    path_res.append(\n",
    "        np.float64(\n",
    "            option_price_binomial(\n",
    "                volatilities=v,\n",
    "                strikes=K,\n",
    "                expiries=T,\n",
    "                spots=S,\n",
    "                discount_rates=r,\n",
    "                dividend_rates=0.0,\n",
    "                is_call_options=True,\n",
    "                is_american=False,\n",
    "                num_steps=100,\n",
    "                dtype=np.float64,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    cfrm_res.append(black_scholes_model(S=S, K=K, T=T, r=r, v=v, o=0))\n",
    "    v += 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dual_line(\n",
    "    x=vols,\n",
    "    y1=path_res,\n",
    "    y2=cfrm_res,\n",
    "    title=\"Path Dependant va Closed Form - Same Option & Parameters\",\n",
    "    x_lab=\"Volatility\",\n",
    "    y1_lab=\"Path Dependant\",\n",
    "    y2_lab=\"Closed Form\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences are relativly small, as the close form is just a different way of calculating the same thing\n",
    "# however, it can deal with more complex cases than the closed form.\n",
    "#\n",
    "np.asarray(path_res) - np.asarray(cfrm_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e0982",
   "metadata": {},
   "source": [
    "## Extend feature set\n",
    "\n",
    "We need to extend the feature set to cover the option style, Call or Put and early exercise or not, American or European\n",
    "\n",
    "These string features needed encoding as numerical values so they can be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features without type and style.\n",
    "vols, mats, spots, strikes, rates = parameter_scenarios(num_steps=20)\n",
    "X = generate_model_scenarios(spot=spots, strike=strikes, mat=mats, rate=rates, vol=vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a84b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all combinations with option type\n",
    "# Call = 0, Put = 1\n",
    "X = combinations(X, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all combinations with option style\n",
    "# American (early exercise) = 0, European = 1\n",
    "X = combinations(X, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c3b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training data\n",
    "#\n",
    "print(\n",
    "    f\"There are now {len(X):,} training data points, for same grid size but with added features\"\n",
    ")\n",
    "Xs = shuffle(X, random_state=42)\n",
    "dfxs = pd.DataFrame(\n",
    "    Xs,\n",
    "    columns=[\n",
    "        \"spot\",\n",
    "        \"strike\",\n",
    "        \"maturity\",\n",
    "        \"rate\",\n",
    "        \"volatility\",\n",
    "        \"call_put\",\n",
    "        \"amer_eur\",\n",
    "    ],\n",
    ")\n",
    "dfxs.to_csv(\"XPathDependant.csv\", encoding=\"ascii\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir XPathDependant.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenarios(X_train: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Use the binomial option pricing model to price every scenrio passed in X_Train\n",
    "    Args:\n",
    "        X_train (np.ndarray): numpy array where each row is Spot, Strike, Time to maturity, risk free rate, volatility, is_call, is_american\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Spot, Strike, Time to maturity, risk free rate, volatility, is_call, is_american, Price\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    num_rows, num_cols = np.shape(X_train)\n",
    "    S, K, T, r, v, cp, ae = np.hsplit(X_train, num_cols)  # Split by column as model can price in bulk on GPU as it is written in Tensor Flow.\n",
    "    d = np.zeros((num_rows, 1)) # Div yield is always zero for our purpose.\n",
    "\n",
    "    prices = option_price_binomial(\n",
    "        volatilities=v,\n",
    "        strikes=K,\n",
    "        expiries=T,\n",
    "        spots=S,\n",
    "        discount_rates=r,\n",
    "        dividend_rates=d,\n",
    "        is_call_options=np.logical_not(cp),\n",
    "        is_american=np.logical_not(ae),\n",
    "        num_steps=200,\n",
    "        dtype=np.float64)\n",
    "    \n",
    "    return np.hstack((X_train, prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new CSV file for results and write a header row.\n",
    "\n",
    "# This data is super expensive to re-create so the file create from scratch is commented out\n",
    "\n",
    "#f = open(\"binomial_training.csv\", \"w\")\n",
    "#f.write(f\"Spot, Strike, Maturity, rate, volatility, isPut, isEuropean, price\\n\")\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934675f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the pre-shuffled pandas X_train data in blocks, writing results out as we\n",
    "# go. There are millions of training examples, so we want to make sure results are saved\n",
    "# down. So, fo each block we flush, close and re-open the file in append mode.\n",
    "base = 0\n",
    "sz = 100000\n",
    "all = len(dfxs)\n",
    "\n",
    "while base < all:\n",
    "    # Open in append.\n",
    "    f = open(\"binomial_training.csv\", \"a\")\n",
    "\n",
    "    pdx = dfxs.iloc[base : base + sz]\n",
    "    X_train = (pdx).to_numpy()\n",
    "    results = run_scenarios(X_train)\n",
    "    for r in results:\n",
    "        S, K, T, r, v, cp, ae, p = r\n",
    "        f.write(f\"{S},{K},{T},{r},{v},{cp},{ae},{p}\\n\")\n",
    "    f.flush()\n",
    "    f.close()\n",
    "    base = base + sz\n",
    "    print(f\"{base:,} of {all:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs some clean post split up as it splits lines\n",
    "#\n",
    "def split_file(filename: str, size_in_MB: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    split the given file into smaller files of the given size\n",
    "\n",
    "    :param filename     : The name of the file to split\n",
    "    :param size_in_MB   : Size in MB of the chunked files\n",
    "\n",
    "    :credit to: https://stackoverflow.com/users/239247/anatoly-techtonik\n",
    "    \"\"\"\n",
    "    MAX = size_in_MB * 1024 * 1024\n",
    "    BUF = 50 * 1024 * 1024 * 1024  # 50GB   - memory buffer size\n",
    "    chapters = 0\n",
    "    uglybuf = \"\"\n",
    "    with open(filename, \"rb\") as src:\n",
    "        while True:\n",
    "            tgt = open(filename + \".%03d\" % chapters, \"wb\")\n",
    "            written = 0\n",
    "            while written < MAX:\n",
    "                if len(uglybuf) > 0:\n",
    "                    tgt.write(uglybuf)\n",
    "                tgt.write(src.read(min(BUF, MAX - written)))\n",
    "                written += min(BUF, MAX - written)\n",
    "                uglybuf = src.read(1)\n",
    "                if len(uglybuf) == 0:\n",
    "                    break\n",
    "            tgt.close()\n",
    "            if len(uglybuf) == 0:\n",
    "                break\n",
    "            chapters += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d537dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file(\"binomial_training_copy.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
